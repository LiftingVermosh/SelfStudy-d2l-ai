# Softmax回归的从零开始实现

## 1.本节直接实现了基于数学定义softmax运算的softmax函数。这可能会导致什么问题？

softmax函数的数学定义为：

$$softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^K e^{x_j}}$$

在特征值过小的时候，softmax函数会出现下溢出，导致输出结果为0。
在特征值过大的时候，softmax函数会出现上溢出，导致输出结果为无穷大。

## 2. 本节中的函数cross_entropy是根据交叉熵损失函数的定义实现的。它可能有什么问题？提示：考虑对数的定义域

交叉熵损失函数的定义为：

$$H(p,q)=-\sum_{i=1}^n p_i \log q_i$$

其中，$p$和$q$分别是真实概率分布和预测概率分布。

当$p_i$或$q_i$为0时，$\log q_i$或$\log p_i$可能出现无效值，导致计算结果为无穷大或无穷小。

## 3. 请想一个解决方案来解决上述两个问题

解决方案：

- 针对Softmax: 输入归一化
- 针对cross_entropy: 加入一个很小的常数，如1e-10，使得$\log q_i$或$\log p_i$不会出现无效值。

## 4. 返回概率最大的分类标签总是最优解吗？例如，医疗诊断场景下可以这样做吗？

并非，因为softmax回归的目标是学习到一个概率分布，而不是直接预测分类标签。在医疗诊断场景下，我们可能希望预测患者患有某种疾病的概率，而不是直接预测患者是否患有某种疾病。

## 5. 假设我们使用softmax回归来预测下一个单词，可选取的单词数目过多可能会带来哪些问题

过多的单词数目可能会导致softmax函数的计算结果过于稀疏，导致模型的泛化能力较差
