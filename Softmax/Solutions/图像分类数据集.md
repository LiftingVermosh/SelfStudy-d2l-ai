# 图像分类数据集

## 1.减少batch_size（如减少到1）是否会影响读取性能？

    减少batch_size会影响读取性能，因为每次读取的数据量会减少，这会导致训练速度变慢。但是，如果训练集的大小足够大，则可以考虑减少batch_size。

## 数据迭代器的性能非常重要。当前的实现足够快吗？探索各种选择来改进它

    当前的数据迭代实现为：

    ```python
    def get_dataloader_workers():
    """使用4个进程来读取数据"""
    return 4

    def load_data_fashion_mnist(batch_size, resize=None):
    """下载Fashion-MNIST数据集，然后将其加载到内存中"""
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(root='../data', train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(root='../data', train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=get_dataloader_workers()))
    ```

    这个实现使用了`num_workers`参数来设置读取数据的进程数。默认情况下，它设置为4，这意味着会使用4个进程来读取数据。

    这个实现的性能非常好，因为它使用了多进程来读取数据，这使得读取速度更快。但是，如果数据集很大，或者网络很复杂，那么这个实现可能成为瓶颈。

    为了改进这个实现，可以尝试以下方法：

    1. 使用多线程来读取数据。使用`num_workers=0`来禁用多进程，并使用`num_workers=4`来启用多线程。
    2. 使用异步数据读取。使用`num_workers=0`来禁用多进程，并使用`pin_memory=True`来将数据直接放入GPU内存中。
    3. 使用缓存。使用`cache=True`来缓存数据集，这样可以避免重复读取相同的数据。
    4. 使用更快的图像预处理方法。例如，可以使用`transforms.RandomResizedCrop`来随机裁剪图像，而不是使用`transforms.Resize`。

## 3.还有哪些其他数据集可用？

    除了Fashion-MNIST，还有其他数据集可供选择。例如，CIFAR-10、CIFAR-100、ImageNet、MNIST等。这些数据集的大小和复杂度都有所不同，但都可以用于图像分类任务。
