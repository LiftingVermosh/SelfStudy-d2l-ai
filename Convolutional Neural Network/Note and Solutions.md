# Chapter 6 卷积神经网络 - Convolutional Neural Networks

## 6.1 从全连接层到卷积

### 6.1.1 不变性

CV中的神经网络架构原则应当包括：

- 平移不变性：一个物体在图像中的位置发生变化（平移）时，网络识别出该物体的能力应该保持不变
  - 实现:权值共享的卷积操作，即通过较小的卷积核来捕捉局部特征，然后在全局上进行特征整合
- 局部性：网络在处理的早期阶段，应该只关注输入数据的局部、小范围区域，而不是一开始就试图理解全局  
  - 实现:通过池化层来降低输入的空间分辨率，从而减少网络的计算量

### 6.1.2 多层感知机的限制

一般而言，MLP会将输入展平，这显然完全破坏了输入之间的空间邻近关系，而且这会以 $O(n^m)$ 的空间复杂度扩展参数维，不仅会让训练时间大幅增长，而且模型会拼命记住训练数据中的噪声而非一般规律,导致过拟合

#### 平移不变性

书中从MLP的全连接层的形式出发，导出了平移不变性，具体而言：

假设输入是一张展平（flattened）后的图像，表示为一个列向量 $x$，其维度为 $[D, 1]$，一个隐藏层的输出 $h$（维度为 $[M, 1]$）计算如下：$$h = \sigma(Wx + b)$$其中：

- $W$ 是权重矩阵，维度为 $[M, D]$。
- $b$ 是偏置向量，维度为 $[M, 1]$。
- $\sigma$ 是非线性激活函数。

考虑输出 $h$ 的第 $m$ 个神经元的值 $h_m$：$$h_m = \sigma( \sum_{d=1}^{D} W_{m,d} \times x_d + b_m )$$求和项 $\sum_{d=1}^{D} W_{m,d} \times x_d$ 意味着神经元 $h_m$ 的激活值依赖于所有输入像素 $x_d$。权重 $W_{m,d}$ 将第 $m$ 个神经元与第 $d$ 个像素固定且全局地连接。公式本身没有体现出任何“相邻像素应该更相关”的先验知识。正如我们前面所说，图像的**空间拓扑结构在输入被拉平时就已经丢失**了。
更进一步的说，假设图像中有一个特征（比如一个边缘），它最初出现在位置 (i, j)，对应拉平后的索引为 d。如果我们将这个特征平移到新的位置 (i+Δi, j+Δj)，对应拉平后的索引为 d’。

- 对于原特征，其激活贡献主要来自于 $W_{m,d} \times x_d$。
- 对于平移后的特征，其激活贡献将变为 $W_{m,d'} \times x_{d'}$。
由于 $W_{m,d}$ 和 $W_{m,d'}$ 是两个完全不同、独立学习的权重参数，它们的值八成不相等。因此，即使输入的特征完全相同（只是位置变了），它对神经元 $h_m$ 的激活贡献也会发生剧烈变化，导致 $h_m$ 的值完全不同。

而将输入保持为2维的情况下：对于维度为 $[H,W]$的二维输入 $X$ ,在卷积核 $K=[k,k]$ 作用下，输出 $Y$ 的第 $(i,j)$ 个元素 $y(i, j)$ 计算如下：
$$Y(i, j) = \sigma( \sum_{a=0}^{k-1} \sum_{b=0}^{k-1} K(a, b) \times X(i+a, j+b) + b )$$
类比求和项 $\sum\limits_{a=0}^{k-1}\sum\limits_{b=0}^{k-1}$ ,表明输出 $Y(i, j)$ 的值仅依赖于输入 $X$ 在以 $(i, j)$ 为中心的 $k\times k$ 局部窗口内的像素进行平移操作，将特征从位置 $(i, j)$ 平移到 $(i+\Delta i, j+\Delta j)$， 此时 $y(i+\Delta i, j+\Delta j)$ 的输出为：
$$y(i+\Delta i, j+\Delta j) = \sigma( \sum_{a=0}^{k-1} \sum_{b=0}^{k-1} K(a, b) \times X(i+a+\Delta i, j+b+\Delta j) + b )$$很明显，卷积核 $K(a, b)$ 并没有发生改变，但其作用区域发生了变化，从 $i\times j$ 变为了 $(i + \Delta i)\times (j + \Delta j)$，这就引入了平移不变性，**实现了特征与位置的解耦**。

> CNN中的 '卷积' 操作，在数学定义上实际是 '互相关' (*Cross-Correlation*)
>
> 但其实第一次看到这个词，我想起的是概统中的卷积，两者在思想上似乎有类比之处：
>
> - 一个函数（或信号/图像）在另一个函数（卷积核）滑过它时，所得到的重叠区域的加权和。
>
> 问了下Deepseek，他给出的类比包括：
>
> | 操作 | 概率论中的卷积 | CNN中的卷积（互相关） |
> | :--- | :--- | :--- |
> | **核心操作** | 滑动、加权求和 | 滑动、加权求和（点积） |
> | **第一个函数** | 一个随机变量的概率分布 $f$ | 输入图像或特征图 |
> | **第二个函数** | 另一个随机变量的概率分布 $g$ | 卷积核（滤波器） |
> | **“滑动”目的** | 计算所有可能的（X, Y）组合如何贡献于Z=z | 计算卷积核与图像所有局部区域的匹配程度 |
> | **输出** | 新随机变量Z的概率分布 $h(z)$ | 新的特征图（激活图） |
>
> 那么从这个角度出发，两者都是滑动加权求和的过程，CNN卷积可以看作离散版本的二维卷积/互相关

#### 局部性

局部性其实在上述推导过程中已经有相关体现了，具体而言，我们限定 $K(a, b)$ 仅与 $X$ 在以 $(i, j)$ 为中心的 $k\times k$ 局部窗口内的像素有关，这就保证了输出 $Y(i, j)$ 只依赖于输入 $X$ 在以 $(i, j)$ 为中心的 $k\times k$ 局部窗口内的像素进行平移操作，从而实现了局部性。

从我个人的理解来说，CNN是通过训练得到一组（多个）卷积核，每个核都是一个专门的特征提取器；在前向传播时，输入数据会经过所有这些提取器的并行“扫描”和“匹配”，生成一组特征图；网络后续的层级则对这些提取到的特征进行组合、抽象和最终决策。

### 6.1.3 卷积

前面提到过，概统中也有卷积的概念，具体而言，其被定义为：$$(f*g)(n) = \int_{-\infty}^{\infty} f(m)g(n-m)dm$$当对象离散时，卷积的定义可以写成：$$\begin{aligned}
(f*g)(n) &= \sum_{m=-\infty}^{\infty} f(m)g(n-m) \\
&= \sum_{m=-\infty}^{\infty} f(m)g(n+m) \\
&= (f*g)(n+k)
\end{aligned}$$其中，$f$ 和 $g$ 是两个函数，$n$ 是任意整数。连等的原因是因为我们总可以通过符号匹配实现+-的转换。

### 6.1.4 推广到图像

实际上，图像一般包含RGB三个通道，此时我们的输入变成了 $[H, W, C] (C\ for\ Channels)$ 的三维张量，相应的，卷积核也应当变成 $[k_h, k_w, c_{in}]$ 的三维张量，其中 $c_{in}$ 是输入通道数。
此时的输出 $Y$ 也应当是 $[H_o, W_o, c_{out}]$ 的三维张量，其中 $H_o$ 和 $W_o$ 是输出的高和宽，$c_{out}$ 是输出通道数，也是该卷积层所拥有的卷积核的数量，它决定了该层学习到的特征的丰富程度
那么卷积核张量最终是四维的，即 $[k_h, k_w, c_{in}, c_{out}]$，即 `[卷积核高度, 卷积核宽度, 输入通道数, 输出通道数]` ,其中 $c_{in}$ 和 $c_{out}$ 一般是相同的。

更精确一点，推导过程可以是：

定义：

- 输入张量: $X \in \mathbb{R}^{H_{in} \times W_{in} \times C_{in}}$
- 卷积核张量: $V \in \mathbb{R}^{k_h \times k_w \times C_{in} \times C_{out}}$
- 偏置向量: $b \in \mathbb{R}^{C_{out}}$ (每个输出通道有一个偏置)
- 输出张量: $H \in \mathbb{R}^{H_{out} \times W_{out} \times C_{out}}$

输出 $H$ 在位置 $(i, j)$ 和第 d 个通道上的值计算公式为：$$H(i, j, d) = \sigma \left( \underbrace{\sum_{a=0}^{k_h-1} \sum_{b=0}^{k_w-1} \sum_{c=0}^{C_{in}-1} V(a, b, c, d) \cdot X(i+a, j+b, c)}_{\text{卷积操作}} + b(d) \right)$$

- $\sum_{a=0}^{k_h-1} \sum_{b=0}^{k_w-1}$: 在空间维度上，对卷积核的$ k_h \times k_w $个位置进行遍历。
- $\sum_{c=0}^{C_{in}-1}$: 在通道维度上，对所有的输入通道 $C_in$ 进行遍历并求和。这意味着卷积核 $V[:, :, :, d]$ 的每个2D切片 $V[:, :, c, d]$ 与输入 $X[:, :, c]$ 的对应通道进行卷积，然后将所有 $C_in$ 个通道的卷积结果相加，融合成一个数值。这就是多通道信息融合的过程。
- $V(a, b, c, d) \cdot X(i+a, j+b, c)$: 卷积核在 $(a,b,c,d)$ 位置的权重与输入在 $(i+a, j+b, c)$ 位置的像素值相乘。
- $b(d)$: 加上第 d 个输出通道的偏置项。
- $\sigma(\dots)$: 非线性激活函数（如ReLU）

> &#8195;
> **如何理解：卷积核三维,但是卷积核张量是四维张量?**
>
> **一个**处理多通道输入的卷积核也是三维的：它有高度（$k_h$）、宽度（$k_w$）和深度（$C_in$，与输入通道数一致）
> 三维卷积核的功用是：在**所有输入通道**上同时滑动，提取一种特定的空间模式（比如一个“左上-右下”的斜边）
> 那么为了描述一个卷积核，我们需要**四个**坐标：
>
> - 高度（$k_h$）、宽度（$k_w$）：描述了卷积核的形状
> - 输入通道数（$C_{in}$）：描述了输入的通道数
> - 输出通道数（$C_{out}$）：描述了输出的通道数
>
> 所以，我们可以这么理解：
>
> - “卷积核是三维的”：指的是单个特征提取器本身的维度。
> - “卷积核张量是四维的”：指的是存储整个卷积层所有特征提取器的容器的维度。
> - 如果说卷积核张量是书柜的话，那么卷积核就是书架上的书

### Exercises 6.1

#### 假设卷积层 (6.1.3)覆盖的局部区域 $\Delta = 0$ 。在这种情况下，证明卷积内核为每组通道独立地实现一个全连接层

在标准卷积神经网络中，卷积操作涉及一个四维卷积核张量 $ V $ 形状为 $[k_h, k_w, C_{\text{in}}, C_{\text{out}}]$，其中：

- $ k_h $ 和 $ k_w $ 是卷积核的空间尺寸（高度和宽度）。
- $ C_{\text{in}} $ 是输入通道数。
- $ C_{\text{out}} $ 是输出通道数。

对于输入张量 $ X $ 形状为 $[H, W, C_{\text{in}}]$，输出张量 $ H $ 形状为 $[H', W', C_{\text{out}}]$，卷积操作定义为：
$$
H(i, j, m) = \sigma \left( \sum_{a=0}^{k_h-1} \sum_{b=0}^{k_w-1} \sum_{c=0}^{C_{\text{in}}-1} V(a, b, c, m) \cdot X(i+a, j+b, c) + b(m) \right)
$$
其中 $ \sigma $ 是激活函数，$ b $ 是偏置向量。

当卷积核大小为1x1时（Δ=0），在这种情况下，卷积核张量 $ V $ 的形状变为 $[1, 1, C_{\text{in}}, C_{\text{out}}]$。这意味着：

- 卷积核不再覆盖空间邻域（因为 $ a $ 和 $ b $ 只能为0）。
- 卷积操作简化为只对通道维度进行线性组合。

因此，输出公式简化为：
$$
H(i, j, m) = \sigma \left( \sum_{c=0}^{C_{\text{in}}-1} V(0, 0, c, m) \cdot X(i, j, c) + b(m) \right)
$$

现在，考虑每个空间位置 $(i, j)$。输入在该位置是一个向量 $ \mathbf{x}_{i,j} \in \mathbb{R}^{C_{\text{in}}} $，其中 $ \mathbf{x}_{i,j} = [X(i,j,0), X(i,j,1), \dots, X(i,j,C_{\text{in}}-1)] $。输出在该位置是一个向量 $ \mathbf{h}_{i,j} \in \mathbb{R}^{C_{\text{out}}} $，其中 $ \mathbf{h}_{i,j} = [H(i,j,0), H(i,j,1), \dots, H(i,j,C_{\text{out}}-1)] $。

定义权重矩阵 $ W \in \mathbb{R}^{C_{\text{out}} \times C_{\text{in}}} $，其中 $ W(m, c) = V(0, 0, c, m) $，和偏置向量 $ \mathbf{b} \in \mathbb{R}^{C_{\text{out}}} $。则对于每个空间位置 $(i, j)$，有：
$$
\mathbf{h}_{i,j} = \sigma \left( W \mathbf{x}_{i,j} + \mathbf{b} \right)
$$这正好是全连接层的操作：对输入向量进行线性变换并激活。

#### 为什么平移不变性可能也不是好主意呢？

平移不变性可能也不是好主意。因为它不关心物体的绝对位置，会主动丢弃位置信息，但在位置信息存在作用的情况下，就成了明显缺陷

#### 当从图像边界像素获取隐藏表示时，我们需要思考哪些问题？

当从图像边界像素获取隐藏表示时，我们需要考虑以下问题：

- 边界像素的位置信息是否重要？
- 如何处理边界像素的位置信息？
- 信息可能存在部分缺失，如何处理？
- 处理效率如何？
- 当前边界处理方式是否会引入新的、不被我们希望的特征？

#### 描述一个类似的音频卷积层的架构

| 特性 | 图像卷积 (2D) | 音频一维卷积 (1D) | 音频二维卷积 (2D on Spectrogram) |
| :--- | :--- | :--- | :--- |
| **输入维度** | `[H, W, C_in]` | `[T, C_in]` | `[T, F, C_in]` (Time, Frequency, Channels) |
| **卷积核** | `[k_h, k_w, C_in, C_out]` | `[k_t, C_in, C_out]` | `[k_t, k_f, C_in, C_out]` |
| **感受野** | 空间局部区域 (patch) | **时间**局部区域 (snippet) | **时-频**局部区域 (patch) |
| **滑动方向** | 沿**高度**和**宽度** | 沿**时间**轴 | 沿**时间**和**频率**轴 |
| **平移不变性** | **空间**平移不变性 | **时间**平移不变性 | **时-频**平移不变性 |

#### 卷积层也适合于文本数据吗？为什么？

能，但未必最优。
我们可以将文本转换成适合卷积操作的结构，就像图像是像素矩阵

- 每个单词或子词被映射为一个密集向量（词向量），维度为 d
- 一个长度为 L 的句子就可以表示为一个 2D 矩阵，形状为 [L, d]
- 与图像使用 2D 卷积核（在高和宽上滑动）不同，文本卷积通常使用 一维卷积
- 卷积核的宽度等于词向量的维度 d，高度（或称“窗口大小”）为 k。这意味着它一次查看 k 个连续的单词。
  例如，一个 k=3 的卷积核，其形状为 [3, d]。它会在序列方向（即时间轴，从一个词到下一个词）上滑动，每次计算这 3 个单词向量的一个加权组合，从而产生一个新的特征。多个这样的卷积核可以检测句子中不同的局部模式。

通过卷积，我们能够捕捉到文本的局部模式，如语法和语义，但视野受限于卷积层大小的限制。因此，卷积层可能不适合处理长文本，如文章、微博等。
此外，正如前文提及，卷积的过程中往往会导致位置丢失，即它无法区分一个短语是出现在句首还是句尾，而这有时对语义至关重要。
还有池化(见后文)，虽然池化往往用来降维和整合特征。能最大化的提取最显著的特征，但它也丢弃了序列的顺序和精细的时间信息。

#### 证明在 $(f*g)(i,j)=\sum\limits_{n=-\infty}^\infty\sum\limits_{m=-\infty}^{\infty} f(m, n)g(i-m,j-n)$ 中，$f * g = g * f$

假设卷积定义为：
$$
(f * g)(i, j) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f(m, n) \, g(i - m, j - n)
$$
我们需要证明 $(f * g)(i, j) = (g * f)(i, j)$.

首先，考虑 $(g * f)(i, j)$ 的定义：
$$
(g * f)(i, j) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} g(m, n) \, f(i - m, j - n)
$$

现在，进行变量替换。令 $k = i - m$ 和 $l = j - n$。则当 $m$ 和 $n$ 取遍所有整数时，$k$ 和 $l$ 也取遍所有整数。同时，有 $m = i - k$ 和 $n = j - l$。

代入上式：
$$
(g * f)(i, j) = \sum_{k=-\infty}^{\infty} \sum_{l=-\infty}^{\infty} g(i - k, j - l) \, f(k, l)
$$

由于求和变量是哑变量，可以重命名 $k$ 和 $l$ 回 $m$ 和 $n$：
$$
(g * f)(i, j) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} g(i - m, j - n) \, f(m, n)
$$

比较 $(f * g)(i, j)$：
$$
(f * g)(i, j) = \sum_{m=-\infty}^{\infty} \sum_{n=-\infty}^{\infty} f(m, n) \, g(i - m, j - n)
$$

由于乘法具有交换性，即 $f(m, n) \, g(i - m, j - n) = g(i - m, j - n) \, f(m, n)$，因此两个求和表达式 identical：
$$
(f *g)(i, j) = (g* f)(i, j)
$$

因此，卷积操作是可交换的。
